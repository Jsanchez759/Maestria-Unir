{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from epftoolbox.evaluation import MAE, MAPE, RMSE, rMAE\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')\n",
    "tf.random.set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "# Romper el dataset\n",
    "def train_test_vali(serie, train_size, valida_size, ts_size):\n",
    "    total_data = serie.shape[0]\n",
    "\n",
    "    train_data = int(total_data * train_size)\n",
    "    valid_data = int(total_data * valida_size)\n",
    "    test_data = total_data - train_data - valid_data\n",
    "\n",
    "    train = serie[0:train_data]\n",
    "    vali = serie[train_data:train_data + valid_data]\n",
    "    test = serie[train_data + valid_data:]\n",
    "    return train, vali, test\n",
    "\n",
    "#Crear dataset supervisado\n",
    "def to_sequences(SEQUENCE_SIZE, obs):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(obs)-SEQUENCE_SIZE):\n",
    "        window = obs[i:(i+SEQUENCE_SIZE)]\n",
    "        after_window = obs[i+SEQUENCE_SIZE]\n",
    "        window = [[x] for x in window]\n",
    "        x.append(window)\n",
    "        y.append(after_window)\n",
    "        \n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "# Auxiliares del modelo\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    rmse = tf.math.sqrt(tf.math.reduce_mean(tf.square(y_pred-y_true)))\n",
    "    return rmse\n",
    "\n",
    "# Función para medir el tiempo de ejecución\n",
    "def elapsed_time(start_time):\n",
    "    return time.time() - start_time\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def call_existing_code(head_size, num_heads, ff_dim, dropout, mlp_dropout):\n",
    "    inputs = keras.Input(shape=(24, 1))\n",
    "    x = inputs\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    x = layers.Dense(32, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        loss=root_mean_squared_error,\n",
    "        optimizer=RMSprop(learning_rate=5e-5)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_model(hp):\n",
    "    head_size = hp.Int(\"head_size\", min_value=32, max_value=128, step=32)\n",
    "    num_heads = hp.Choice(\"num_heads\", values=[1, 2])\n",
    "    ff_dim = hp.Int(\"ff_dim\", min_value=1, max_value=3)\n",
    "    dropout = hp.Float(\"dropout\", min_value=0.01, max_value=0.5)\n",
    "    mlp_dropout = hp.Float(\"mlp_dropout\", min_value=0.01, max_value=0.5)\n",
    "    model = call_existing_code(head_size, num_heads, ff_dim, dropout, mlp_dropout)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 09s]\n",
      "val_loss: 0.005203812848776579\n",
      "\n",
      "Best val_loss So Far: 0.005203812848776579\n",
      "Total elapsed time: 00h 01m 09s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      " \n",
      "Training the best model\n",
      "Epoch 1/50\n",
      "5583/5583 [==============================] - 61s 10ms/step - loss: 0.0094 - val_loss: 0.0056\n",
      "Epoch 2/50\n",
      "5583/5583 [==============================] - 56s 10ms/step - loss: 0.0076 - val_loss: 0.0049\n",
      "Epoch 3/50\n",
      "5583/5583 [==============================] - 53s 9ms/step - loss: 0.0072 - val_loss: 0.0055\n",
      "Epoch 4/50\n",
      "5583/5583 [==============================] - 56s 10ms/step - loss: 0.0071 - val_loss: 0.0055\n",
      "Epoch 5/50\n",
      "5583/5583 [==============================] - 54s 10ms/step - loss: 0.0068 - val_loss: 0.0048\n"
     ]
    }
   ],
   "source": [
    "# Crear diferentes modelos base para cada tipo de particion\n",
    "types = [(0.9,0.05,0.05)]\n",
    "lista_results = []\n",
    "INPUT_LENGTH = 24    # Registros de 24 horas consecutivas a la entrada\n",
    "OUTPUT_LENGTH = 24   # El modelo va a predecir 24 horas a futuro\n",
    "epochs = 50\n",
    "for i in types:\n",
    "    scaler = MinMaxScaler()\n",
    "    df = pd.read_csv('df_data_colum.csv', parse_dates=['date_hour'], index_col='date_hour')\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    tr, vl, ts = train_test_vali(df_scaled['price'], i[0], i[1], i[2])\n",
    "\n",
    "    x_tr, y_tr = to_sequences(OUTPUT_LENGTH, tr.values)\n",
    "    x_vl, y_vl = to_sequences(OUTPUT_LENGTH, vl.values)\n",
    "    x_ts, y_ts = to_sequences(OUTPUT_LENGTH, ts.values)\n",
    "\n",
    "    tuner = keras_tuner.BayesianOptimization(\n",
    "        hypermodel=build_model,\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=30,\n",
    "        executions_per_trial=1,\n",
    "        overwrite=True,\n",
    "        directory=\"enconder_model\",\n",
    "        project_name=\"tunning_model\"\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0003)\n",
    "    tuner.search(x_tr, y_tr, epochs=1, validation_data=(x_vl, y_vl), callbacks=[early_stopping])\n",
    "\n",
    "    architecture = tuner.get_best_hyperparameters(5)[0]\n",
    "\n",
    "    archi = []\n",
    "    for j in [\"head_size\", \"num_heads\", \"ff_dim\", \"dropout\", \"mlp_dropout\"]:\n",
    "        archi.append((architecture.get(j), j))\n",
    "\n",
    "    model = build_model(architecture)\n",
    "    print(\" \")\n",
    "    print(\"Training the best model\")\n",
    "    start_time = time.time()\n",
    "    model.fit(x_tr, y_tr, epochs=epochs, validation_data=(x_vl, y_vl), callbacks=[early_stopping])\n",
    "    trainig_time = elapsed_time(start_time)\n",
    "\n",
    "    rmse_tr = model.evaluate(x_tr, y_tr, verbose=1)\n",
    "    rmse_vl = model.evaluate(x_vl, y_vl, verbose=1)\n",
    "    rmse_ts = model.evaluate(x_ts, y_ts, verbose=1)\n",
    "\n",
    "    #Grafico de entrenamiento\n",
    "    # df_history = pd.DataFrame(historia.history)\n",
    "    # x = df_history.index\n",
    "    # plt.figure(figsize=(15, 10))\n",
    "    # plt.plot(x, df_history['loss'], label='Función de perdida en entrenamiento')\n",
    "    # plt.plot(x, df_history['val_loss'], label='Función de perdida en validación')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    start_time = time.time()\n",
    "    y_ts_pred = model.predict(x_ts)\n",
    "    prediction_time = elapsed_time(start_time)\n",
    "    test = pd.DataFrame(data = {'predictions':y_ts_pred.reshape(-1), 'actual':y_ts}, index=ts[24:].index)\n",
    "    df_result = pd.DataFrame(scaler.inverse_transform(test), columns=test.columns)\n",
    "\n",
    "    # Grafico de predicciones\n",
    "    x = df_result.index\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(x, df_result['actual'], label='actual')\n",
    "    plt.plot(x, df_result['predictions'], label='predictions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Grafico de error \n",
    "    df_result['error'] = df_result['actual'] - df_result['predictions']\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(df_result['error'])\n",
    "    plt.show()\n",
    "\n",
    "    # Grafico de RMSE por hora\n",
    "    df_result.index = test.index\n",
    "    df_result['hora'] = df_result.index.hour\n",
    "    rmse_por_hora = []\n",
    "    for hora in range(24):\n",
    "        df_hora = df_result[df_result['hora'] == hora]\n",
    "        rmse = np.sqrt(mean_squared_error(df_hora['actual'], df_hora['predictions']))\n",
    "        rmse_por_hora.append(rmse)\n",
    "\n",
    "    rmse_df = pd.DataFrame(rmse_por_hora, columns=['rmse_por_hora'], index=range(len(rmse_por_hora)))\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.plot(rmse_df['rmse_por_hora'], marker='o')\n",
    "    ax.set_xlabel('Hora predicha')\n",
    "    ax.set_ylabel('Error RMSE ($/kWh)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Guardar el modelo\n",
    "    model.save('enconder_model/tunning_save/best_model_{train_size}.h5'.format(train_size=i[0]))\n",
    "\n",
    "    mean_MAE = MAE(p_pred= df_result['predictions'], p_real=df_result['actual'])\n",
    "    mean_RMSE = RMSE(p_pred= df_result['predictions'], p_real=df_result['actual'])\n",
    "    mean_MAPE = MAPE(p_pred= df_result['predictions'], p_real=df_result['actual']) * 100\n",
    "    r2 = metrics.r2_score(df_result['actual'], df_result['predictions'])\n",
    "\n",
    "    results = {'trainig_size': i[0], 'model': 'transformer', 'type': 'tunning', 'training_time': trainig_time, \n",
    "               'prediction_time': prediction_time, 'rmse_trainig': rmse_tr, 'rmse_validation': rmse_vl, 'rmse_test': rmse_ts, \n",
    "               'mean_MAE': mean_MAE, 'r2': r2, 'mean_RMSE': mean_RMSE, 'mean_MAPE': mean_MAPE, \"architecture\": [archi]}\n",
    "    \n",
    "    lista_results.append(results)\n",
    "\n",
    "df_results_bases = pd.DataFrame(lista_results)\n",
    "df_results_bases.to_csv('enconder_model/tunning_save/df_results_best_{train}.csv'.format(train=i[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
